# AlexNet Example

## Description

These are examples of training AlexNet with dataset in MindSpore.

## Requirements

- Install [MindSpore](https://www.mindspore.cn/install/en).

- Download the dataset, the directory structure is as follows:

{% if dataset=='Cifar10' %}
Cifar10

```
└─Data
    ├─test
    │      cifar-10-verify-bin
    │
    └─train
           cifar-10-batches-bin
```

{% elif dataset=='ImageNet' %}
ImageNet

```
└─Data
    ├─test
    │       validation_preprocess
    │
    └─train
            ilsvrc
```
{% endif %}

## Structure

```shell
.
└──alexnet
  ├── README.md
  ├── script
    ├── run_distribute_train.sh            # launch distributed training(8 pcs)
    ├── run_eval.sh                        # launch evaluation
    ├── run_standalone_train.sh            # launch standalone training(1 pcs)
    ├── run_distribute_train_gpu.sh        # launch gpu distributed training(4 pcs)
    ├── run_eval_gpu.sh                    # launch gpu evaluation
    └── run_standalone_train_gpu.sh        # launch gpu standalone training(1 pcs)
  ├── src
    ├── config.py                          # parameter configuration
    ├── dataset.py                         # data preprocessing
    ├── generator_lr.py                    # generate learning rate for each step
    └── alexnet.py                         # alexnet network definition
  ├── eval.py                              # eval net
  └── train.py                             # train net
```


## Parameter configuration

Parameters for both training and evaluation can be set in src/config.py.


## Running the example

### Running on Ascend

#### Train

##### Usage

```
# distributed training
Usage: bash run_distribute_train.sh [RANK_TABLE_FILE] [DATASET_PATH] [PRETRAINED_CKPT_PATH](optional)

# standalone training
Usage: bash run_standalone_train.sh [DATASET_PATH] [PRETRAINED_CKPT_PATH](optional)
```


##### Launch

```
# distribute training example
bash run_distribute_train.sh rank_table.json ~/cifar-10-batches-bin

# standalone training example
bash run_standalone_train.sh ~/cifar-10-batches-bin
```

> About rank_table.json, you can refer to the [distributed training tutorial](https://www.mindspore.cn/tutorial/training/zh-CN/master/advanced_use/distributed_training_ascend.html).

##### Result

Training result will be stored in the example path, whose folder name begins with "train" or "train_parallel". Under this, you can find checkpoint file together with result like the followings in log.

```
epoch: 1 step: 1, loss is 2.3041954
epoch: 1 step: 2, loss is 2.3079312
...
epoch: 1 step: 601, loss is 2.314184
epoch: 1 step: 603, loss is 2.305666
...
```

#### Evaluation

##### Usage

```
# evaluation
Usage: bash run_eval.sh [DATASET_PATH] [CHECKPOINT_PATH]
```

##### Launch

```
# evaluation example
bash run_eval.sh ~/cifar-10-verify-bin ~/resnet50/train/alexnet-1.591.ckpt
```

> checkpoint can be produced in training process.


### Running on GPU
```
# distributed training example
bash run_distribute_train_gpu.sh [DATASET_PATH] [PRETRAINED_CKPT_PATH](optional)

# standalone training example
bash run_standalone_train_gpu.sh [DATASET_PATH] [PRETRAINED_CKPT_PATH](optional)

# infer example
bash run_eval_gpu.sh [DATASET_PATH] [CHECKPOINT_PATH]
```
