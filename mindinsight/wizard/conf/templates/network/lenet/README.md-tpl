# LeNet Example

## Description

These are examples of training LeNet with dataset in MindSpore.

## Requirements

- Install [MindSpore](https://www.mindspore.cn/install/en).

- Download the dataset, the directory structure is as follows:

```
└─Data
    ├─test
    │      t10k-images.idx3-ubyte
    │      t10k-labels.idx1-ubyte
    │
    └─train
           train-images.idx3-ubyte
           train-labels.idx1-ubyte
```

## Structure

```shell
.
└──lenet
  ├── README.md
  ├── script
    ├── run_distribute_train.sh            # launch distributed training(8 pcs)
    ├── run_eval.sh                        # launch evaluation
    ├── run_standalone_train.sh            # launch standalone training(1 pcs)
    ├── run_distribute_train_gpu.sh        # launch gpu distributed training(8 pcs)
    ├── run_eval_gpu.sh                    # launch gpu evaluation
    └── run_standalone_train_gpu.sh        # launch gpu standalone training(1 pcs)
  ├── src
    ├── config.py                          # parameter configuration
    ├── dataset.py                         # data preprocessing
    └── lenet.py                           # lenet network definition
  ├── eval.py                              # eval net
  └── train.py                             # train net
```


## Parameter configuration

Parameters for both training and evaluation can be set in src/config.py.


## Running the example

### Running on Ascend

#### Train

##### Usage

```
# distributed training
Usage: bash run_distribute_train.sh [RANK_TABLE_FILE] [DATASET_PATH] [PRETRAINED_CKPT_PATH](optional)

# standalone training
Usage: bash run_standalone_train.sh [DATASET_PATH] [PRETRAINED_CKPT_PATH](optional)
```


##### Launch

```
# distribute training example
bash run_distribute_train.sh rank_table.json ~/MNIST_data

# standalone training example
bash run_standalone_train.sh ~/MNIST_data
```

> About rank_table.json, you can refer to the [distributed training tutorial](https://www.mindspore.cn/tutorial/training/zh-CN/master/advanced_use/distributed_training_ascend.html).

##### Result

Training result will be stored in the example path, whose folder name begins with "train" or "train_parallel". Under this, you can find checkpoint file together with result like the followings in log.

```
epoch: 1 step: 1, loss is 2.3041954
epoch: 1 step: 2, loss is 2.3079312
...
epoch: 1 step: 601, loss is 2.314184
epoch: 1 step: 603, loss is 2.305666
...
```

#### Evaluation

##### Usage

```
# evaluation
Usage: bash run_eval.sh [DATASET_PATH] [CHECKPOINT_PATH]
```

##### Launch

```
# evaluation example
bash run_eval.sh ~/MNIST_data ~/lenet/train_parallel0/ckpt_0/checkpoint_lenet-2_937.ckpt
```

> checkpoint can be produced in training process.


### Running on GPU
```
# distributed training example
bash run_distribute_train_gpu.sh [DATASET_PATH] [PRETRAINED_CKPT_PATH](optional)

# standalone training example
bash run_standalone_train_gpu.sh [DATASET_PATH] [PRETRAINED_CKPT_PATH](optional)

# infer example
bash run_eval_gpu.sh [DATASET_PATH] [CHECKPOINT_PATH]
```
